# -*- coding: utf-8 -*-
"""ViT Model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CudVBsKrnApXGZKuYQMW1E-bUy9oK123
"""

from google.colab import drive
drive.mount('/content/drive')

from IPython import get_ipython
from IPython.display import display
# %%
# %%
!pip install transformers datasets --quiet
# %%
from transformers import ViTFeatureExtractor, TFViTModel
import tensorflow as tf
import numpy as np
import os
import cv2
from sklearn.model_selection import train_test_split
from sklearn.utils import shuffle
from tensorflow.keras.utils import to_categorical
import matplotlib.pyplot as plt
# %%

# Load the ViT feature extractor
feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224-in21k')
# %%
IMAGE_SIZE = 224
LABELS = ['glioma', 'meningioma', 'notumor', 'pituitary']
NUM_CLASSES = len(LABELS)
BASE_PATH = '/content/drive/MyDrive/Final Year Project'

# Step 1: Load raw images
def load_images(base_path, labels, image_size, max_per_class=500):
    X, Y = [], []
    for label in labels:
        count = 0
        for folder in ['Training', 'Testing']:
            folder_path = os.path.join(base_path, folder, label)
            if not os.path.exists(folder_path):
                continue
            for file in os.listdir(folder_path):
                if count >= max_per_class:
                    break
                img_path = os.path.join(folder_path, file)
                img = cv2.imread(img_path)
                if img is not None:
                    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
                    img = cv2.resize(img, (image_size, image_size))
                    X.append(img)
                    Y.append(labels.index(label))
                    count += 1
    return np.array(X), to_categorical(Y, num_classes=len(labels))

# Load images
X_data, Y_data = load_images(BASE_PATH, LABELS, IMAGE_SIZE)
X_data, Y_data = shuffle(X_data, Y_data, random_state=42)
# %%

# Step 2: Preprocess using Hugging Face's feature extractor (outside the model)
# This converts the images to the format expected by the ViT model ( channels, height, width)
# and normalizes pixel values.
X_data = feature_extractor(images=list(X_data), return_tensors="np")["pixel_values"]

# Step 3: Split
X_train, X_test, y_train, y_test = train_test_split(X_data, Y_data, test_size=0.1, random_state=42)
# %%

# Load ViT backbone
vit_model = TFViTModel.from_pretrained('google/vit-base-patch16-224-in21k')

# Freeze the ViT model weights
vit_model.trainable = False
# %%
# Step 4: Extract features using the pre-trained ViT model
# Process X_train and X_test through the vit_model to get features
# Use batch processing to avoid out-of-memory issues for large datasets
# Step 4: Extract features using the pre-trained ViT model
# Process X_train and X_test through the vit_model to get features
# Use batch processing to avoid out-of-memory issues for large datasets

def extract_features(model, data, batch_size=32):
    features = []
    # Ensure the data is in the correct format (NumPy array or tf.Tensor)
    data = tf.constant(data, dtype=tf.float32)
    dataset = tf.data.Dataset.from_tensor_slices(data).batch(batch_size)
    print(f"Processing {tf.data.experimental.cardinality(dataset).numpy()} batches...")
    for i, batch in enumerate(dataset):
        # Get the pooled output for each batch
        batch_features = model(batch, training=False).pooler_output # Use training=False when extracting features
        features.append(batch_features.numpy())
        # Optional: print progress
        if (i + 1) % 10 == 0:
            print(f"Processed batch {i + 1}")
    return np.concatenate(features, axis=0)

print("Extracting training features...")
X_train_features = extract_features(vit_model, X_train)
print(f"Training features shape: {X_train_features.shape}")

print("Extracting testing features...")
X_test_features = extract_features(vit_model, X_test)
print(f"Testing features shape: {X_test_features.shape}")

# Step 5: Build and train a classifier on the extracted features
# The input shape for this classifier is the dimension of the extracted features
feature_dim = X_train_features.shape[1] # This should be 768 for vit-base

model = tf.keras.Sequential([
    tf.keras.layers.Input(shape=(feature_dim,)), # Input shape matches the feature dimension (768,)
    tf.keras.layers.Dense(256, activation='relu'),
    tf.keras.layers.Dropout(0.3),
    tf.keras.layers.Dense(NUM_CLASSES, activation='softmax')
])

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
model.summary()

# Step 6: Train the classifier
print("Training the classifier...")
history = model.fit(X_train_features, y_train, validation_split=0.1, epochs=10, batch_size=32)

# Evaluate the model on the test set
print("Evaluating the model...")
loss, accuracy = model.evaluate(X_test_features, y_test, batch_size=32)
print(f"Test Loss: {loss}")
print(f"Test Accuracy: {accuracy}")

import matplotlib.pyplot as plt

# Accuracy plot
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Val Accuracy')
plt.title('Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.grid(True)
plt.show()

# Loss plot
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Val Loss')
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)
plt.show()

def show_vit_attention(image, model, feature_extractor):
    # Resize to 224x224
    image_resized = cv2.resize(image, (224, 224))

    # Fix channels
    if image_resized.ndim == 2:
        image_rgb = cv2.cvtColor(image_resized, cv2.COLOR_GRAY2RGB)
    elif image_resized.shape[2] == 1:
        image_rgb = cv2.cvtColor(image_resized, cv2.COLOR_GRAY2RGB)
    elif image_resized.shape[2] == 3:
        image_rgb = cv2.cvtColor(image_resized, cv2.COLOR_BGR2RGB)
    else:
        raise ValueError("Unexpected image shape:", image_resized.shape)

    # Preprocess image
    inputs = feature_extractor(images=image_rgb, return_tensors="tf")

    # Forward pass with attentions
    outputs = model(**inputs, output_attentions=True)
    attentions = outputs.attentions

    # Take last layer, average heads, focus on [CLS]
    last_attention = attentions[-1]
    attention = tf.reduce_mean(last_attention, axis=1)
    cls_attention = attention[0, 0, 1:]

    # Convert to square map
    num_patches = int(len(cls_attention) ** 0.5)
    cam = tf.reshape(cls_attention, (num_patches, num_patches)).numpy()
    cam = cv2.resize(cam, (224, 224))
    cam = (cam - cam.min()) / (cam.max() - cam.min())

    # Overlay
    heatmap = cv2.applyColorMap(np.uint8(255 * cam), cv2.COLORMAP_JET)
    overlayed = cv2.addWeighted(image_resized, 0.6, heatmap, 0.4, 0)

import cv2
import numpy as np
import matplotlib.pyplot as plt
from PIL import Image
from transformers import ViTModel, ViTFeatureExtractor

# Load model and feature extractor
model = TFViTModel.from_pretrained('google/vit-base-patch16-224-in21k', output_attentions=True)
feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224-in21k')

# Load and preprocess image
image_path = '/content/drive/MyDrive/Final Year Project/Testing/pituitary/Te-piTr_0009.jpg'  # <-- replace this with your image path
image = Image.open(image_path).convert("RGB").resize((224, 224))
image_np = np.array(image)
image_rgb = image_np.copy()  # for later visualization

# Preprocess for model
inputs = feature_extractor(images=image_np, return_tensors="tf")
outputs = model(**inputs)

# Get attention from the last block
attentions = outputs.attentions[-1]  # shape: (1, num_heads, num_tokens, num_tokens)
cls_attn = attentions[0, :, 0, 1:]  # CLS token to all patches

# Average attention heads
cam = tf.reduce_mean(cls_attn, axis=0).numpy().reshape(14, 14)  # ViT has 14x14 patches for 224x224

# Normalize attention
cam = cv2.resize(cam, (224, 224))
cam = (cam - cam.min()) / (cam.max() - cam.min())  # normalize between 0-1

# Overlay on original image
heatmap = cv2.applyColorMap(np.uint8(255 * cam), cv2.COLORMAP_JET)
overlayed = cv2.addWeighted(image_rgb, 0.6, heatmap, 0.4, 0)

# Plot
plt.figure(figsize=(12, 4))

plt.subplot(1, 3, 1)
plt.imshow(image_rgb)
plt.title("Original")
plt.axis('off')

plt.subplot(1, 3, 2)
plt.imshow(cam, cmap='jet')
plt.title("Attention Map")
plt.axis('off')

plt.subplot(1, 3, 3)
plt.imshow(overlayed)
plt.title("Overlay")
plt.axis('off')

plt.tight_layout()
plt.show()